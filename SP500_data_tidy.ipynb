{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZYZXa78czm0f66rLgTVo8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aUJ843qfAfDe","executionInfo":{"status":"ok","timestamp":1758119789937,"user_tz":-480,"elapsed":2577,"user":{"displayName":"æŽå®—ç©Ž","userId":"06893125105939496608"}},"outputId":"d9f365ce-430f-402e-c8eb-831edc99b434"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","/content/drive/MyDrive/LSTM_PROGRAM\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","%cd /content\n","\n","# åˆ‡åˆ°ä½ çš„å°ˆæ¡ˆè³‡æ–™å¤¾\n","%cd /content/drive/MyDrive/LSTM_PROGRAM"]},{"cell_type":"code","source":["!mkdir -p ~/.ssh\n","!cp /content/drive/MyDrive/.ssh/id_ed25519* ~/.ssh/\n","!chmod 700 ~/.ssh\n","!chmod 600 ~/.ssh/id_ed25519\n","\n","!eval \"$(ssh-agent -s)\" && ssh-add ~/.ssh/id_ed25519\n","!ssh-keyscan github.com >> ~/.ssh/known_hosts\n","!chmod 644 ~/.ssh/known_hosts\n","\n","!ssh -T git@github.com\n","\n","!git config --global user.email \"joemi7878@gmail.com\"\n","\n","\n","!pip install arch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mjt2nvfjAxSR","executionInfo":{"status":"ok","timestamp":1758120114720,"user_tz":-480,"elapsed":12217,"user":{"displayName":"æŽå®—ç©Ž","userId":"06893125105939496608"}},"outputId":"057053d8-0ca8-462a-d99d-2712ce6752a1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Agent pid 8565\n","Identity added: /root/.ssh/id_ed25519 (joemi7878@gmail.com)\n","# github.com:22 SSH-2.0-1588e33\n","# github.com:22 SSH-2.0-cb24e083\n","# github.com:22 SSH-2.0-cb24e083\n","# github.com:22 SSH-2.0-1588e33\n","# github.com:22 SSH-2.0-1588e33\n","Hi joemi78! You've successfully authenticated, but GitHub does not provide shell access.\n","Requirement already satisfied: arch in /usr/local/lib/python3.12/dist-packages (7.2.0)\n","Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from arch) (2.0.2)\n","Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.12/dist-packages (from arch) (1.16.1)\n","Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.12/dist-packages (from arch) (2.2.2)\n","Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.12/dist-packages (from arch) (0.14.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4->arch) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4->arch) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4->arch) (2025.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.12->arch) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.12->arch) (25.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->arch) (1.17.0)\n"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import re\n","from functools import reduce\n","\n","# 2. ã€ç”¨æˆ¶è‡ªå®šç¾©å€é–“ã€‘\n","start_date = '2004/03/26'\n","end_date = '2025/07/01'\n","start = pd.to_datetime(start_date)\n","end = pd.to_datetime(end_date)\n","\n","# è·¯å¾‘å’Œæª”å\n","folder = '.'\n","csv_files_day = [\n","    # './source_data/BTCUSDT_DAY.csv',\n","    # './source_data/ETHUSDT_DAY.csv',\n","    './source_data/CME_MINI_DL_ES1!, 1D.csv',\n","    # './source_data/CME_MINI_DL_NQ1!, 1D.csv',\n","    # './source_data/CBOT_MINI_DL_YM1!, 1D.csv',\n","    './source_data/CBOE_DLY_VX1!, 1D.csv',\n","    # './source_data/CBOT_DL_ZN1!, 1D.csv',\n","    # './source_data/CBOT_DL_ZT1!, 1D.csv',\n","    # './source_data/CBOT_DL_ZF1!, 1D.csv',\n","    # './source_data/CBOT_DL_TN1!, 1D.csv',\n","    # './source_data/CBOT_DL_ZB1!, 1D.csv',\n","    # './source_data/CME_DL_SR31!, 1D.csv',\n","#    './source_data/TVC_US10Y, 1D.csv',\n","    # './source_data/COMEX_DL_GC1!, 1D.csv',\n","#    './source_data/BITSTAMP_DAIUSD, 1D.csv',\n","    # './source_data/BITSTAMP_USDCUSD, 1D.csv',\n","    # './source_data/BITSTAMP_USDTUSD, 1D.csv'\n","#    './source_data/TAIFEX_DLY_TXF1!, 1D.csv'\n","]\n","\n","csv_files_hour = [\n","    './source_data/BTCUSDT_HOUR.csv',\n","    './source_data/ETHUSDT_HOUR.csv',\n","    './source_data/CME_MINI_DL_ES1!, 60.csv',\n","    './source_data/CME_MINI_DL_NQ1!, 60.csv',\n","    './source_data/CBOT_MINI_DL_YM1!, 60.csv',\n","    './source_data/CBOE_DLY_VX1!, 60.csv',\n","    './source_data/CBOT_DL_ZN1!, 60.csv',\n","    './source_data/CBOT_DL_ZT1!, 60.csv',\n","    './source_data/CBOT_DL_ZF1!, 60.csv',\n","    './source_data/CBOT_DL_ZB1!, 60.csv',\n","    './source_data/CBOT_DL_TN1!, 60.csv',\n","    './source_data/CME_DL_SR31!, 60.csv',\n","#    './source_data/TVC_US10Y, 60.csv',\n","    './source_data/COMEX_DL_GC1!, 60.csv',\n","#    './source_data/BITSTAMP_DAIUSD, 60.csv',\n","    './source_data/Bitstamp_USDCUSD_1h_cleaned.csv',\n","    './source_data/Bitstamp_USDTUSD_1h_cleaned.csv'\n","# #    './source_data/TAIFEX_DLY_TXF1!, 60.csv'\n","]\n","def find_date_col(df):\n","    for col in df.columns:\n","        if 'date' in col.lower():\n","            return col\n","    return df.columns[0]\n","\n","\n","def get_prefix(file):\n","    if 'BTC' in file: return 'BTCUSDT'\n","    if 'ETH' in file: return 'ETHUSDT'\n","#    if 'DAI' in file: return 'DAIUSD'\n","    if 'USDCUSD' in file: return 'USDCUSD'\n","    if 'USDTUSD' in file: return 'USDTUSD'\n","    if 'ES1' in file: return 'ES1'\n","    if 'NQ1' in file: return 'NQ1'\n","    if 'YM1' in file: return 'YM1'\n","    if 'VX1' in file: return 'VIX'\n","    if 'ZN1' in file: return 'ZN1'\n","    if 'ZT1' in file: return 'ZT1'\n","    if 'ZF1' in file: return 'ZF1'\n","    if 'ZB1' in file: return 'ZB1'\n","    if 'TN1' in file: return 'TN1'\n","    if 'SR3' in file: return 'SR3'\n","#    if 'US10Y' in file: return 'US10Y'\n","    if 'GC1' in file: return 'GC1'\n","#    if 'DAI' in file: return 'DAIUSD'\n","    if 'USDCUSD' in file: return 'USDCUSD'\n","    if 'USDTUSD' in file: return 'USDTUSD'\n","#    if 'TXF1' in file: return 'TXF1'\n","    # fallback\n","    return os.path.splitext(os.path.basename(file))[0]\n","\n","# ä½ ä¹‹å¾Œå†å‘¼å« get_prefix(file) æ‰ä¸æœƒå‡ºéŒ¯\n","def impute_and_flag(df, start, end, freq, asset_prefix):\n","    # \"\"\"\n","    # df: å–®ä¸€è³‡ç”¢çš„ DataFrameï¼ˆå·²ç¶“æŠŠæ—¥æœŸæ¬„æ”¹åç‚º 'DATE'ï¼Œå…¶ä»–æ¬„å·²åŠ ä¸Šè³‡ç”¢å‰ç¶´ï¼‰\n","    # start, end: datetime-likeï¼Œå¯ç”¨ä½ ä¸Šé¢å®šç¾©çš„ start/end\n","    # freq: 'D' æˆ– 'H'\n","    # asset_prefix: ä¾‹å¦‚ 'ES1'ã€'BTCUSDT'ã€'ZN1'â€¦ï¼ˆç”¨ä¾†å‘½å is_trading æ¬„ä½ï¼‰\n","    # \"\"\"\n","    # 1) æ•´ç†æ™‚é–“æ¬„ä½ä¸¦æŽ’åº\n","    df = df.copy()\n","    df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')\n","    df = df.dropna(subset=['DATE']).sort_values('DATE')\n","    df = df.loc[(df['DATE'] >= start) & (df['DATE'] <= end)]\n","\n","    # 2) å»ºç«‹å®Œæ•´ç´¢å¼•ä¸¦ reindex\n","    full_index = pd.date_range(start=start, end=end, freq=freq)\n","    df = df.set_index('DATE')\n","    original_idx = df.index\n","\n","    df = df.reindex(full_index)\n","\n","    # 3) is_trading æ¬„ï¼šåŽŸå§‹æœ‰åˆ—=1ï¼›è£œå‡ºçš„=0\n","    flag_col = f\"{asset_prefix}_is_trading\"\n","    df[flag_col] = 0\n","    df.loc[original_idx.intersection(df.index), flag_col] = 1\n","\n","    # 4) é‡å°æ¬„ä½é¡žåž‹è£œå€¼\n","    #   - OHLC â†’ ffill\n","    #   - VOLUME / VOLUME_BASE â†’ 0.0\n","    #   - å…¶ä»– â†’ ffill\n","    for col in df.columns:\n","        if col == flag_col:\n","            continue\n","        # æ³¨æ„ï¼šä½ çš„æ¬„ä½å·²ç¶“å«è³‡ç”¢å‰ç¶´ï¼Œå› æ­¤ç”¨ endswith åˆ¤æ–·å¾Œç¶´\n","        if col.endswith(('_OPEN', '_HIGH', '_LOW', '_CLOSE')):\n","            df[col] = df[col].ffill()\n","            # å¦‚æžœæ•´åˆ—éƒ½é‚„æ˜¯ NaNï¼Œå°±å¡« 0\n","            df[col] = df[col].fillna(0.0)\n","        elif col.endswith(('_VOLUME', '_VOLUME_BASE')):\n","            df[col] = df[col].fillna(0.0)\n","        else:\n","            df[col] = df[col].ffill()\n","            df[col] = df[col].fillna(0.0)\n","    # 5) æŠŠç´¢å¼•é‚„åŽŸç‚º DATE æ¬„ä½ï¼ˆç¶­æŒä½ æ—¢æœ‰æ…£ä¾‹ï¼‰\n","    df = df.reset_index().rename(columns={'index': 'DATE'})\n","\n","    return df\n","\n","def read_and_clean(file):\n","    df = pd.read_csv(file)\n","    date_col = find_date_col(df)\n","    tried = False\n","    for fmt in ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M', '%Y-%m-%d %H:%M', '%Y/%m/%d %H:%M:%S']:\n","        try:\n","            df[date_col] = pd.to_datetime(df[date_col], format=fmt, errors='raise')\n","            tried = True\n","            break\n","        except Exception:\n","            continue\n","    if not tried:\n","        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n","    # å¼·åˆ¶ floor åˆ°å°æ™‚ï¼Œé¿å…äº¤é›†å¤±æ•—\n","    df[date_col] = df[date_col].dt.floor('h')\n","    df[date_col] = df[date_col].dt.tz_localize(None)\n","    df = df.rename(columns={date_col: 'DATE'})\n","    return df\n","\n","dfs_day = []\n","for file in csv_files_day:\n","    df1 = pd.read_csv(os.path.join(folder, file))\n","    date_col1 = find_date_col(df1)\n","    df1[date_col1] = pd.to_datetime(df1[date_col1], errors='coerce')\n","    df1 = df1.dropna(subset=[date_col1])\n","    prefix = get_prefix(file)\n","    # å°‡æ‰€æœ‰éžæ—¥æœŸæ¬„ä½åŠ å‰ç¶´\n","    rename = {col: f\"{prefix}_{col.upper()}\" for col in df1.columns if col != date_col1}\n","    df1 = df1.rename(columns=rename)\n","    df1 = df1.rename(columns={date_col1: 'DATE'})\n","\n","    # ðŸ”½ðŸ”½ðŸ”½ã€æ–°å¢žã€‘æ—¥é »è£œå€¼ + æ——æ¨™ï¼ˆfreq='D'ï¼‰\n","    df1 = impute_and_flag(df1, start, end, freq='D', asset_prefix=prefix)\n","\n","    dfs_day.append(df1)\n","\n","# dfs_hour = []\n","# for file in csv_files_hour:\n","#     df2 = read_and_clean(file)\n","#     date_col2 = find_date_col(df2)\n","#     df2[date_col2] = pd.to_datetime(df2[date_col2], errors='coerce')\n","#     df2 = df2.dropna(subset=[date_col2])\n","#     prefix = get_prefix(file)\n","#     # å°‡æ‰€æœ‰éžæ—¥æœŸæ¬„ä½åŠ å‰ç¶´\n","#     rename = {col: f\"{prefix}_{col.upper()}\" for col in df2.columns if col != date_col2}\n","#     df2 = df2.rename(columns=rename)\n","#     df2 = df2.rename(columns={date_col2: 'DATE'})\n","\n","#     # ðŸ”½ðŸ”½ðŸ”½ã€æ–°å¢žã€‘å°æ™‚è£œå€¼ + æ——æ¨™ï¼ˆfreq='H'ï¼‰\n","#     df2 = impute_and_flag(df2, start, end, freq='h', asset_prefix=prefix)\n","\n","#     dfs_hour.append(df2)\n","\n","# print(f\"dfs_hour List\")\n","# for i, df in enumerate(dfs_hour):\n","#     print(f\"---- dfs_hour[{i}] ----\")\n","#     # print(f\"Shape: {df.shape}\")\n","#     print(\"Columns:\", df.columns.tolist())\n","#     #print(df.head(), \"\\n\")\n","# print(f\"\\ndfs_day List\")\n","# for i, df in enumerate(dfs_day):\n","#     print(f\"---- dfs_day[{i}] ----\")\n","#     # print(f\"Shape: {df.shape}\")\n","#     print(\"Columns:\", df.columns.tolist())\n","#     #print(df.head(), \"\\n\")\n","\n","# 1. prefix â†’ group å¯¹ç…§è¡¨ï¼Œä¸€å®šè¦æŠŠ list_prefixes é‡Œæ‰€æœ‰çš„éƒ½åŠ è¿›æ¥\n","GROUP_MAP = {\n","    'crypto': ['BTCUSDT','ETHUSDT','USDCUSD','USDTUSD'],\n","    'stock':  ['ES1','NQ1','YM1','VIX'],\n","    'bonds':  ['US10Y','ZN1','ZF1','ZT1','ZB1','TN1','SR3'],\n","    'others':  ['GC1']\n","}\n","\n","def classify_dfs(dfs):\n","    groups = { g: [] for g in GROUP_MAP }\n","    for df in dfs:\n","        prefix = get_prefix(df.columns[1].split('_',1)[0])\n","        for grp, prefixes in GROUP_MAP.items():\n","            if prefix in prefixes:\n","                groups[grp].append(df)\n","                break\n","        else:\n","            raise AssertionError(f\"å‰ç¼€ {prefix} æ²¡åˆ†åˆ°ç»„ï¼Œè¯·è¡¥å……åˆ° GROUP_MAP\")\n","    return groups\n","\n","\n","\n","day_groups  = classify_dfs(dfs_day)\n","# hour_groups = classify_dfs(dfs_hour)\n","\n","for name, lst in day_groups.items():\n","    print(f\"Day   / {name:6s}: {len(lst)} æª”\")\n","# for name, lst in hour_groups.items():\n","#     print(f\"Hour  / {name:6s}: {len(lst)} æª”\")\n","\n","\n","\n","# å‡è¨­å‰é¢å·²ç¶“æœ‰ day_groups, hour_groups å…©å€‹ dict:\n","# { 'crypto':[df1,df2,â€¦], 'stock':[â€¦], 'bonds':[â€¦] }\n","merged = {}\n","for name, lst in day_groups.items():\n","    if not lst:  # å¦‚æžœæ¸…å–®æ˜¯ç©ºçš„å°±è·³éŽ\n","        print(f\"âš ï¸ è·³éŽ {name}ï¼ˆæ²’æœ‰ä»»ä½•æª”æ¡ˆï¼‰\")\n","        continue\n","\n","    key = f\"df_merged_day_{name}\"\n","    if len(lst) == 1:\n","        merged[key] = lst[0]  # åªæœ‰ä¸€å¼µè¡¨å°±ç›´æŽ¥å­˜\n","    else:\n","        merged[key] = reduce(\n","            lambda left, right: pd.merge(left, right, on='DATE', how='inner'),\n","            lst\n","        )\n","\n","# merged è£¡å°±æœƒæœ‰\n","# merged['df_merged_day_crypto']\n","# merged['df_merged_day_stock']\n","# merged['df_merged_day_bonds']\n","# merged['df_merged_hour_crypto']\n","# merged['df_merged_hour_stock']\n","# merged['df_merged_hour_bonds']\n","\n","\n","# ä½ æƒ³è¦çš„ prefix é †åºï¼ˆä¸åœ¨è£¡é¢çš„ prefix å‰‡è·³éŽï¼‰\n","preferred_order = [\n","    'BTCUSDT', 'ETHUSDT','USDCUSD', 'USDTUSD',\n","    'ES1', 'NQ1', 'YM1','VIX',\n","    'SR3', 'ZT1', 'ZF1', 'TN1', 'ZN1','ZB1',\n","    'US10Y', 'GC1'\n","]\n","\n","# ç¢ºä¿ output ç›®éŒ„å­˜åœ¨\n","out_dir = \"./filtered_output/row_group/\"\n","os.makedirs(out_dir, exist_ok=True)\n","\n","for name, df in merged.items():\n","    # name æœƒé•·å¾—åƒ \"df_merged_hour_crypto\"ã€\"df_merged_day_stock\"â€¦\n","    # æˆ‘å€‘æŠŠå®ƒæ‹†æˆ freq + groupï¼Œç„¶å¾Œæ‰“å…¥æª”å\n","    _, _, freq, group = name.split(\"_\")\n","    out_path = os.path.join(out_dir, f\"{group}_{freq}_fluctuation_aligned.csv\")\n","\n","    # æŠŠ DATE æ”¾æœ€å‰é¢\n","    cols = [\"DATE\"]\n","\n","    # ç…§ä½ çš„ preferred_orderã€ä»¥åŠ [CLOSE, VOLUME, OPEN, HIGH, LOW] çš„é †åºæŒ‘æ¬„ä½\n","    for suffix in [\"CLOSE\", \"VOLUME\", \"OPEN\", \"HIGH\", \"LOW\", \"is_trading\"]:\n","        for prefix in preferred_order:\n","            col = f\"{prefix}_{suffix}\"\n","            if col in df.columns:\n","                cols.append(col)\n","\n","    # é‡æŽ’æ¬„ä½\n","    df_out = df[cols]\n","\n","    # å¯« CSV\n","    df_out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n","    print(f\"âœ… è¼¸å‡ºå®Œæˆï¼š{out_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qopSp1xUA0Ef","executionInfo":{"status":"ok","timestamp":1758120034830,"user_tz":-480,"elapsed":218,"user":{"displayName":"æŽå®—ç©Ž","userId":"06893125105939496608"}},"outputId":"1adc1b5d-d7eb-4b10-b1e7-9aee69d35abe"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Day   / crypto: 0 æª”\n","Day   / stock : 2 æª”\n","Day   / bonds : 0 æª”\n","Day   / others: 0 æª”\n","âš ï¸ è·³éŽ cryptoï¼ˆæ²’æœ‰ä»»ä½•æª”æ¡ˆï¼‰\n","âš ï¸ è·³éŽ bondsï¼ˆæ²’æœ‰ä»»ä½•æª”æ¡ˆï¼‰\n","âš ï¸ è·³éŽ othersï¼ˆæ²’æœ‰ä»»ä½•æª”æ¡ˆï¼‰\n","âœ… è¼¸å‡ºå®Œæˆï¼š./filtered_output/row_group/stock_day_fluctuation_aligned.csv\n"]}]}]}