{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZYZXa78czm0f66rLgTVo8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aUJ843qfAfDe","executionInfo":{"status":"ok","timestamp":1758119789937,"user_tz":-480,"elapsed":2577,"user":{"displayName":"李宗穎","userId":"06893125105939496608"}},"outputId":"d9f365ce-430f-402e-c8eb-831edc99b434"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","/content/drive/MyDrive/LSTM_PROGRAM\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","%cd /content\n","\n","# 切到你的專案資料夾\n","%cd /content/drive/MyDrive/LSTM_PROGRAM"]},{"cell_type":"code","source":["!mkdir -p ~/.ssh\n","!cp /content/drive/MyDrive/.ssh/id_ed25519* ~/.ssh/\n","!chmod 700 ~/.ssh\n","!chmod 600 ~/.ssh/id_ed25519\n","\n","!eval \"$(ssh-agent -s)\" && ssh-add ~/.ssh/id_ed25519\n","!ssh-keyscan github.com >> ~/.ssh/known_hosts\n","!chmod 644 ~/.ssh/known_hosts\n","\n","!ssh -T git@github.com\n","\n","!git config --global user.email \"joemi7878@gmail.com\"\n","\n","\n","!pip install arch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mjt2nvfjAxSR","executionInfo":{"status":"ok","timestamp":1758120114720,"user_tz":-480,"elapsed":12217,"user":{"displayName":"李宗穎","userId":"06893125105939496608"}},"outputId":"057053d8-0ca8-462a-d99d-2712ce6752a1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Agent pid 8565\n","Identity added: /root/.ssh/id_ed25519 (joemi7878@gmail.com)\n","# github.com:22 SSH-2.0-1588e33\n","# github.com:22 SSH-2.0-cb24e083\n","# github.com:22 SSH-2.0-cb24e083\n","# github.com:22 SSH-2.0-1588e33\n","# github.com:22 SSH-2.0-1588e33\n","Hi joemi78! You've successfully authenticated, but GitHub does not provide shell access.\n","Requirement already satisfied: arch in /usr/local/lib/python3.12/dist-packages (7.2.0)\n","Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from arch) (2.0.2)\n","Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.12/dist-packages (from arch) (1.16.1)\n","Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.12/dist-packages (from arch) (2.2.2)\n","Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.12/dist-packages (from arch) (0.14.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4->arch) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4->arch) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4->arch) (2025.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.12->arch) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.12->arch) (25.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->arch) (1.17.0)\n"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import re\n","from functools import reduce\n","\n","# 2. 【用戶自定義區間】\n","start_date = '2004/03/26'\n","end_date = '2025/07/01'\n","start = pd.to_datetime(start_date)\n","end = pd.to_datetime(end_date)\n","\n","# 路徑和檔名\n","folder = '.'\n","csv_files_day = [\n","    # './source_data/BTCUSDT_DAY.csv',\n","    # './source_data/ETHUSDT_DAY.csv',\n","    './source_data/CME_MINI_DL_ES1!, 1D.csv',\n","    # './source_data/CME_MINI_DL_NQ1!, 1D.csv',\n","    # './source_data/CBOT_MINI_DL_YM1!, 1D.csv',\n","    './source_data/CBOE_DLY_VX1!, 1D.csv',\n","    # './source_data/CBOT_DL_ZN1!, 1D.csv',\n","    # './source_data/CBOT_DL_ZT1!, 1D.csv',\n","    # './source_data/CBOT_DL_ZF1!, 1D.csv',\n","    # './source_data/CBOT_DL_TN1!, 1D.csv',\n","    # './source_data/CBOT_DL_ZB1!, 1D.csv',\n","    # './source_data/CME_DL_SR31!, 1D.csv',\n","#    './source_data/TVC_US10Y, 1D.csv',\n","    # './source_data/COMEX_DL_GC1!, 1D.csv',\n","#    './source_data/BITSTAMP_DAIUSD, 1D.csv',\n","    # './source_data/BITSTAMP_USDCUSD, 1D.csv',\n","    # './source_data/BITSTAMP_USDTUSD, 1D.csv'\n","#    './source_data/TAIFEX_DLY_TXF1!, 1D.csv'\n","]\n","\n","csv_files_hour = [\n","    './source_data/BTCUSDT_HOUR.csv',\n","    './source_data/ETHUSDT_HOUR.csv',\n","    './source_data/CME_MINI_DL_ES1!, 60.csv',\n","    './source_data/CME_MINI_DL_NQ1!, 60.csv',\n","    './source_data/CBOT_MINI_DL_YM1!, 60.csv',\n","    './source_data/CBOE_DLY_VX1!, 60.csv',\n","    './source_data/CBOT_DL_ZN1!, 60.csv',\n","    './source_data/CBOT_DL_ZT1!, 60.csv',\n","    './source_data/CBOT_DL_ZF1!, 60.csv',\n","    './source_data/CBOT_DL_ZB1!, 60.csv',\n","    './source_data/CBOT_DL_TN1!, 60.csv',\n","    './source_data/CME_DL_SR31!, 60.csv',\n","#    './source_data/TVC_US10Y, 60.csv',\n","    './source_data/COMEX_DL_GC1!, 60.csv',\n","#    './source_data/BITSTAMP_DAIUSD, 60.csv',\n","    './source_data/Bitstamp_USDCUSD_1h_cleaned.csv',\n","    './source_data/Bitstamp_USDTUSD_1h_cleaned.csv'\n","# #    './source_data/TAIFEX_DLY_TXF1!, 60.csv'\n","]\n","def find_date_col(df):\n","    for col in df.columns:\n","        if 'date' in col.lower():\n","            return col\n","    return df.columns[0]\n","\n","\n","def get_prefix(file):\n","    if 'BTC' in file: return 'BTCUSDT'\n","    if 'ETH' in file: return 'ETHUSDT'\n","#    if 'DAI' in file: return 'DAIUSD'\n","    if 'USDCUSD' in file: return 'USDCUSD'\n","    if 'USDTUSD' in file: return 'USDTUSD'\n","    if 'ES1' in file: return 'ES1'\n","    if 'NQ1' in file: return 'NQ1'\n","    if 'YM1' in file: return 'YM1'\n","    if 'VX1' in file: return 'VIX'\n","    if 'ZN1' in file: return 'ZN1'\n","    if 'ZT1' in file: return 'ZT1'\n","    if 'ZF1' in file: return 'ZF1'\n","    if 'ZB1' in file: return 'ZB1'\n","    if 'TN1' in file: return 'TN1'\n","    if 'SR3' in file: return 'SR3'\n","#    if 'US10Y' in file: return 'US10Y'\n","    if 'GC1' in file: return 'GC1'\n","#    if 'DAI' in file: return 'DAIUSD'\n","    if 'USDCUSD' in file: return 'USDCUSD'\n","    if 'USDTUSD' in file: return 'USDTUSD'\n","#    if 'TXF1' in file: return 'TXF1'\n","    # fallback\n","    return os.path.splitext(os.path.basename(file))[0]\n","\n","# 你之後再呼叫 get_prefix(file) 才不會出錯\n","def impute_and_flag(df, start, end, freq, asset_prefix):\n","    # \"\"\"\n","    # df: 單一資產的 DataFrame（已經把日期欄改名為 'DATE'，其他欄已加上資產前綴）\n","    # start, end: datetime-like，可用你上面定義的 start/end\n","    # freq: 'D' 或 'H'\n","    # asset_prefix: 例如 'ES1'、'BTCUSDT'、'ZN1'…（用來命名 is_trading 欄位）\n","    # \"\"\"\n","    # 1) 整理時間欄位並排序\n","    df = df.copy()\n","    df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')\n","    df = df.dropna(subset=['DATE']).sort_values('DATE')\n","    df = df.loc[(df['DATE'] >= start) & (df['DATE'] <= end)]\n","\n","    # 2) 建立完整索引並 reindex\n","    full_index = pd.date_range(start=start, end=end, freq=freq)\n","    df = df.set_index('DATE')\n","    original_idx = df.index\n","\n","    df = df.reindex(full_index)\n","\n","    # 3) is_trading 欄：原始有列=1；補出的=0\n","    flag_col = f\"{asset_prefix}_is_trading\"\n","    df[flag_col] = 0\n","    df.loc[original_idx.intersection(df.index), flag_col] = 1\n","\n","    # 4) 針對欄位類型補值\n","    #   - OHLC → ffill\n","    #   - VOLUME / VOLUME_BASE → 0.0\n","    #   - 其他 → ffill\n","    for col in df.columns:\n","        if col == flag_col:\n","            continue\n","        # 注意：你的欄位已經含資產前綴，因此用 endswith 判斷後綴\n","        if col.endswith(('_OPEN', '_HIGH', '_LOW', '_CLOSE')):\n","            df[col] = df[col].ffill()\n","            # 如果整列都還是 NaN，就填 0\n","            df[col] = df[col].fillna(0.0)\n","        elif col.endswith(('_VOLUME', '_VOLUME_BASE')):\n","            df[col] = df[col].fillna(0.0)\n","        else:\n","            df[col] = df[col].ffill()\n","            df[col] = df[col].fillna(0.0)\n","    # 5) 把索引還原為 DATE 欄位（維持你既有慣例）\n","    df = df.reset_index().rename(columns={'index': 'DATE'})\n","\n","    return df\n","\n","def read_and_clean(file):\n","    df = pd.read_csv(file)\n","    date_col = find_date_col(df)\n","    tried = False\n","    for fmt in ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M', '%Y-%m-%d %H:%M', '%Y/%m/%d %H:%M:%S']:\n","        try:\n","            df[date_col] = pd.to_datetime(df[date_col], format=fmt, errors='raise')\n","            tried = True\n","            break\n","        except Exception:\n","            continue\n","    if not tried:\n","        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n","    # 強制 floor 到小時，避免交集失敗\n","    df[date_col] = df[date_col].dt.floor('h')\n","    df[date_col] = df[date_col].dt.tz_localize(None)\n","    df = df.rename(columns={date_col: 'DATE'})\n","    return df\n","\n","dfs_day = []\n","for file in csv_files_day:\n","    df1 = pd.read_csv(os.path.join(folder, file))\n","    date_col1 = find_date_col(df1)\n","    df1[date_col1] = pd.to_datetime(df1[date_col1], errors='coerce')\n","    df1 = df1.dropna(subset=[date_col1])\n","    prefix = get_prefix(file)\n","    # 將所有非日期欄位加前綴\n","    rename = {col: f\"{prefix}_{col.upper()}\" for col in df1.columns if col != date_col1}\n","    df1 = df1.rename(columns=rename)\n","    df1 = df1.rename(columns={date_col1: 'DATE'})\n","\n","    # 🔽🔽🔽【新增】日頻補值 + 旗標（freq='D'）\n","    df1 = impute_and_flag(df1, start, end, freq='D', asset_prefix=prefix)\n","\n","    dfs_day.append(df1)\n","\n","# dfs_hour = []\n","# for file in csv_files_hour:\n","#     df2 = read_and_clean(file)\n","#     date_col2 = find_date_col(df2)\n","#     df2[date_col2] = pd.to_datetime(df2[date_col2], errors='coerce')\n","#     df2 = df2.dropna(subset=[date_col2])\n","#     prefix = get_prefix(file)\n","#     # 將所有非日期欄位加前綴\n","#     rename = {col: f\"{prefix}_{col.upper()}\" for col in df2.columns if col != date_col2}\n","#     df2 = df2.rename(columns=rename)\n","#     df2 = df2.rename(columns={date_col2: 'DATE'})\n","\n","#     # 🔽🔽🔽【新增】小時補值 + 旗標（freq='H'）\n","#     df2 = impute_and_flag(df2, start, end, freq='h', asset_prefix=prefix)\n","\n","#     dfs_hour.append(df2)\n","\n","# print(f\"dfs_hour List\")\n","# for i, df in enumerate(dfs_hour):\n","#     print(f\"---- dfs_hour[{i}] ----\")\n","#     # print(f\"Shape: {df.shape}\")\n","#     print(\"Columns:\", df.columns.tolist())\n","#     #print(df.head(), \"\\n\")\n","# print(f\"\\ndfs_day List\")\n","# for i, df in enumerate(dfs_day):\n","#     print(f\"---- dfs_day[{i}] ----\")\n","#     # print(f\"Shape: {df.shape}\")\n","#     print(\"Columns:\", df.columns.tolist())\n","#     #print(df.head(), \"\\n\")\n","\n","# 1. prefix → group 对照表，一定要把 list_prefixes 里所有的都加进来\n","GROUP_MAP = {\n","    'crypto': ['BTCUSDT','ETHUSDT','USDCUSD','USDTUSD'],\n","    'stock':  ['ES1','NQ1','YM1','VIX'],\n","    'bonds':  ['US10Y','ZN1','ZF1','ZT1','ZB1','TN1','SR3'],\n","    'others':  ['GC1']\n","}\n","\n","def classify_dfs(dfs):\n","    groups = { g: [] for g in GROUP_MAP }\n","    for df in dfs:\n","        prefix = get_prefix(df.columns[1].split('_',1)[0])\n","        for grp, prefixes in GROUP_MAP.items():\n","            if prefix in prefixes:\n","                groups[grp].append(df)\n","                break\n","        else:\n","            raise AssertionError(f\"前缀 {prefix} 没分到组，请补充到 GROUP_MAP\")\n","    return groups\n","\n","\n","\n","day_groups  = classify_dfs(dfs_day)\n","# hour_groups = classify_dfs(dfs_hour)\n","\n","for name, lst in day_groups.items():\n","    print(f\"Day   / {name:6s}: {len(lst)} 檔\")\n","# for name, lst in hour_groups.items():\n","#     print(f\"Hour  / {name:6s}: {len(lst)} 檔\")\n","\n","\n","\n","# 假設前面已經有 day_groups, hour_groups 兩個 dict:\n","# { 'crypto':[df1,df2,…], 'stock':[…], 'bonds':[…] }\n","merged = {}\n","for name, lst in day_groups.items():\n","    if not lst:  # 如果清單是空的就跳過\n","        print(f\"⚠️ 跳過 {name}（沒有任何檔案）\")\n","        continue\n","\n","    key = f\"df_merged_day_{name}\"\n","    if len(lst) == 1:\n","        merged[key] = lst[0]  # 只有一張表就直接存\n","    else:\n","        merged[key] = reduce(\n","            lambda left, right: pd.merge(left, right, on='DATE', how='inner'),\n","            lst\n","        )\n","\n","# merged 裡就會有\n","# merged['df_merged_day_crypto']\n","# merged['df_merged_day_stock']\n","# merged['df_merged_day_bonds']\n","# merged['df_merged_hour_crypto']\n","# merged['df_merged_hour_stock']\n","# merged['df_merged_hour_bonds']\n","\n","\n","# 你想要的 prefix 順序（不在裡面的 prefix 則跳過）\n","preferred_order = [\n","    'BTCUSDT', 'ETHUSDT','USDCUSD', 'USDTUSD',\n","    'ES1', 'NQ1', 'YM1','VIX',\n","    'SR3', 'ZT1', 'ZF1', 'TN1', 'ZN1','ZB1',\n","    'US10Y', 'GC1'\n","]\n","\n","# 確保 output 目錄存在\n","out_dir = \"./filtered_output/row_group/\"\n","os.makedirs(out_dir, exist_ok=True)\n","\n","for name, df in merged.items():\n","    # name 會長得像 \"df_merged_hour_crypto\"、\"df_merged_day_stock\"…\n","    # 我們把它拆成 freq + group，然後打入檔名\n","    _, _, freq, group = name.split(\"_\")\n","    out_path = os.path.join(out_dir, f\"{group}_{freq}_fluctuation_aligned.csv\")\n","\n","    # 把 DATE 放最前面\n","    cols = [\"DATE\"]\n","\n","    # 照你的 preferred_order、以及 [CLOSE, VOLUME, OPEN, HIGH, LOW] 的順序挑欄位\n","    for suffix in [\"CLOSE\", \"VOLUME\", \"OPEN\", \"HIGH\", \"LOW\", \"is_trading\"]:\n","        for prefix in preferred_order:\n","            col = f\"{prefix}_{suffix}\"\n","            if col in df.columns:\n","                cols.append(col)\n","\n","    # 重排欄位\n","    df_out = df[cols]\n","\n","    # 寫 CSV\n","    df_out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n","    print(f\"✅ 輸出完成：{out_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qopSp1xUA0Ef","executionInfo":{"status":"ok","timestamp":1758120034830,"user_tz":-480,"elapsed":218,"user":{"displayName":"李宗穎","userId":"06893125105939496608"}},"outputId":"1adc1b5d-d7eb-4b10-b1e7-9aee69d35abe"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Day   / crypto: 0 檔\n","Day   / stock : 2 檔\n","Day   / bonds : 0 檔\n","Day   / others: 0 檔\n","⚠️ 跳過 crypto（沒有任何檔案）\n","⚠️ 跳過 bonds（沒有任何檔案）\n","⚠️ 跳過 others（沒有任何檔案）\n","✅ 輸出完成：./filtered_output/row_group/stock_day_fluctuation_aligned.csv\n"]}]}]}