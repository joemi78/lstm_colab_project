{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHo5Oh15hxWLohmCwk6+Bp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","%cd /content\n","\n","# 切到你的專案資料夾\n","%cd /content/drive/MyDrive/LSTM_PROGRAM"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U0EzXBw9rbXd","executionInfo":{"status":"ok","timestamp":1757173150312,"user_tz":-480,"elapsed":20970,"user":{"displayName":"李宗穎","userId":"06893125105939496608"}},"outputId":"987fec49-da25-44b9-e6ee-3ef316e6b9c8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content\n","/content/drive/MyDrive/LSTM_PROGRAM\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqy66mJorBUn","executionInfo":{"status":"ok","timestamp":1757173170570,"user_tz":-480,"elapsed":16020,"user":{"displayName":"李宗穎","userId":"06893125105939496608"}},"outputId":"e37b964a-4666-4725-ce24-27bbfe94661d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['DATE', 'SR3_CLOSE', 'ZT1_CLOSE', 'ZF1_CLOSE', 'TN1_CLOSE', 'ZN1_CLOSE',\n","       'ZB1_CLOSE', 'SR3_VOLUME', 'ZT1_VOLUME', 'ZF1_VOLUME', 'TN1_VOLUME',\n","       'ZN1_VOLUME', 'ZB1_VOLUME', 'SR3_OPEN', 'ZT1_OPEN', 'ZF1_OPEN',\n","       'TN1_OPEN', 'ZN1_OPEN', 'ZB1_OPEN', 'SR3_HIGH', 'ZT1_HIGH', 'ZF1_HIGH',\n","       'TN1_HIGH', 'ZN1_HIGH', 'ZB1_HIGH', 'SR3_LOW', 'ZT1_LOW', 'ZF1_LOW',\n","       'TN1_LOW', 'ZN1_LOW', 'ZB1_LOW', 'SR3_IS_TRADING', 'ZT1_IS_TRADING',\n","       'ZF1_IS_TRADING', 'TN1_IS_TRADING', 'ZN1_IS_TRADING', 'ZB1_IS_TRADING'],\n","      dtype='object')\n","Index(['DATE', 'SR3_CLOSE', 'ZT1_CLOSE', 'ZF1_CLOSE', 'TN1_CLOSE', 'ZN1_CLOSE',\n","       'ZB1_CLOSE', 'SR3_VOLUME', 'ZT1_VOLUME', 'ZF1_VOLUME', 'TN1_VOLUME',\n","       'ZN1_VOLUME', 'ZB1_VOLUME', 'SR3_OPEN', 'ZT1_OPEN', 'ZF1_OPEN',\n","       'TN1_OPEN', 'ZN1_OPEN', 'ZB1_OPEN', 'SR3_HIGH', 'ZT1_HIGH', 'ZF1_HIGH',\n","       'TN1_HIGH', 'ZN1_HIGH', 'ZB1_HIGH', 'SR3_LOW', 'ZT1_LOW', 'ZF1_LOW',\n","       'TN1_LOW', 'ZN1_LOW', 'ZB1_LOW', 'SR3_IS_TRADING', 'ZT1_IS_TRADING',\n","       'ZF1_IS_TRADING', 'TN1_IS_TRADING', 'ZN1_IS_TRADING', 'ZB1_IS_TRADING'],\n","      dtype='object')\n","Index(['DATE', 'BTCUSDT_CLOSE', 'ETHUSDT_CLOSE', 'USDCUSD_CLOSE',\n","       'USDTUSD_CLOSE', 'BTCUSDT_VOLUME', 'ETHUSDT_VOLUME', 'USDCUSD_VOLUME',\n","       'USDTUSD_VOLUME', 'BTCUSDT_OPEN', 'ETHUSDT_OPEN', 'USDCUSD_OPEN',\n","       'USDTUSD_OPEN', 'BTCUSDT_HIGH', 'ETHUSDT_HIGH', 'USDCUSD_HIGH',\n","       'USDTUSD_HIGH', 'BTCUSDT_LOW', 'ETHUSDT_LOW', 'USDCUSD_LOW',\n","       'USDTUSD_LOW', 'BTCUSDT_IS_TRADING', 'ETHUSDT_IS_TRADING',\n","       'USDCUSD_IS_TRADING', 'USDTUSD_IS_TRADING'],\n","      dtype='object')\n","Index(['DATE', 'BTCUSDT_CLOSE', 'ETHUSDT_CLOSE', 'USDCUSD_CLOSE',\n","       'USDTUSD_CLOSE', 'BTCUSDT_VOLUME', 'ETHUSDT_VOLUME', 'USDCUSD_VOLUME',\n","       'USDTUSD_VOLUME', 'BTCUSDT_OPEN', 'ETHUSDT_OPEN', 'USDCUSD_OPEN',\n","       'USDTUSD_OPEN', 'BTCUSDT_HIGH', 'ETHUSDT_HIGH', 'USDCUSD_HIGH',\n","       'USDTUSD_HIGH', 'BTCUSDT_LOW', 'ETHUSDT_LOW', 'USDCUSD_LOW',\n","       'USDTUSD_LOW', 'BTCUSDT_IS_TRADING', 'ETHUSDT_IS_TRADING',\n","       'USDCUSD_IS_TRADING', 'USDTUSD_IS_TRADING'],\n","      dtype='object')\n","Index(['DATE', 'ES1_CLOSE', 'NQ1_CLOSE', 'YM1_CLOSE', 'VIX_CLOSE',\n","       'ES1_VOLUME', 'NQ1_VOLUME', 'YM1_VOLUME', 'VIX_VOLUME', 'ES1_OPEN',\n","       'NQ1_OPEN', 'YM1_OPEN', 'VIX_OPEN', 'ES1_HIGH', 'NQ1_HIGH', 'YM1_HIGH',\n","       'VIX_HIGH', 'ES1_LOW', 'NQ1_LOW', 'YM1_LOW', 'VIX_LOW',\n","       'ES1_IS_TRADING', 'NQ1_IS_TRADING', 'YM1_IS_TRADING', 'VIX_IS_TRADING'],\n","      dtype='object')\n","Index(['DATE', 'ES1_CLOSE', 'NQ1_CLOSE', 'YM1_CLOSE', 'VIX_CLOSE',\n","       'ES1_VOLUME', 'NQ1_VOLUME', 'YM1_VOLUME', 'VIX_VOLUME', 'ES1_OPEN',\n","       'NQ1_OPEN', 'YM1_OPEN', 'VIX_OPEN', 'ES1_HIGH', 'NQ1_HIGH', 'YM1_HIGH',\n","       'VIX_HIGH', 'ES1_LOW', 'NQ1_LOW', 'YM1_LOW', 'VIX_LOW',\n","       'ES1_IS_TRADING', 'NQ1_IS_TRADING', 'YM1_IS_TRADING', 'VIX_IS_TRADING'],\n","      dtype='object')\n","Index(['DATE', 'GC1_CLOSE', 'GC1_VOLUME', 'GC1_OPEN', 'GC1_HIGH', 'GC1_LOW',\n","       'GC1_IS_TRADING'],\n","      dtype='object')\n","Index(['DATE', 'GC1_CLOSE', 'GC1_VOLUME', 'GC1_OPEN', 'GC1_HIGH', 'GC1_LOW',\n","       'GC1_IS_TRADING'],\n","      dtype='object')\n","bonds: day 有 (1161, 37)，hour 有 (27841, 37)\n","❌ bonds – 排除掉不在日线中的小时点，共 0 行 → ./output1/row_group/bonds_hourly_exclude.csv\n","✅ bonds – 清洗后小时线对齐日线，共 27841 行 → ./output1/row_group/bonds_hourly_clean.csv\n","crypto: day 有 (1161, 25)，hour 有 (27841, 25)\n","❌ crypto – 排除掉不在日线中的小时点，共 0 行 → ./output1/row_group/crypto_hourly_exclude.csv\n","✅ crypto – 清洗后小时线对齐日线，共 27841 行 → ./output1/row_group/crypto_hourly_clean.csv\n","stock: day 有 (1161, 25)，hour 有 (27841, 25)\n","❌ stock – 排除掉不在日线中的小时点，共 0 行 → ./output1/row_group/stock_hourly_exclude.csv\n","✅ stock – 清洗后小时线对齐日线，共 27841 行 → ./output1/row_group/stock_hourly_clean.csv\n","others: day 有 (1161, 7)，hour 有 (27841, 7)\n","❌ others – 排除掉不在日线中的小时点，共 0 行 → ./output1/row_group/others_hourly_exclude.csv\n","✅ others – 清洗后小时线对齐日线，共 27841 行 → ./output1/row_group/others_hourly_clean.csv\n","✅ bonds 小时数据 (27769 rows) 已保存到 ./filtered_output/bonds_hour_clean_period.csv\n","✅ crypto 小时数据 (27769 rows) 已保存到 ./filtered_output/crypto_hour_clean_period.csv\n","✅ stock 小时数据 (27769 rows) 已保存到 ./filtered_output/stock_hour_clean_period.csv\n","✅ others 小时数据 (27769 rows) 已保存到 ./filtered_output/others_hour_clean_period.csv\n","✅ bonds 日线数据 (1158 rows) 已保存到 ./filtered_output/bonds_day_clean_period.csv\n","✅ crypto 日线数据 (1158 rows) 已保存到 ./filtered_output/crypto_day_clean_period.csv\n","✅ stock 日线数据 (1158 rows) 已保存到 ./filtered_output/stock_day_clean_period.csv\n","✅ others 日线数据 (1158 rows) 已保存到 ./filtered_output/others_day_clean_period.csv\n"]}],"source":["import os\n","import pandas as pd\n","import re\n","from functools import reduce\n","from collections import defaultdict\n","\n","# 2. 【用戶自定義區間】\n","start_date = '2022/05/01'\n","end_date = '2025/07/01'\n","start = pd.to_datetime(start_date)\n","end = pd.to_datetime(end_date)\n","\n","\n","# 路徑和檔名\n","folder = '.'\n","csv_files = [\n","    './filtered_output/row_group/bonds_day_aligned.csv',\n","    './filtered_output/row_group/bonds_hour_aligned.csv',\n","    './filtered_output/row_group/crypto_day_aligned.csv',\n","    './filtered_output/row_group/crypto_hour_aligned.csv',\n","    './filtered_output/row_group/stock_day_aligned.csv',\n","    './filtered_output/row_group/stock_hour_aligned.csv',\n","    './filtered_output/row_group/others_day_aligned.csv',\n","    './filtered_output/row_group/others_hour_aligned.csv'\n","]\n","\n","def find_date_col(df):\n","    for col in df.columns:\n","        if 'date' in col.lower() or 'time' in col.lower():\n","            return col\n","    return df.columns[0]\n","\n","def get_prefix(file):\n","    if 'bonds_day' in file: return 'bonds_day'\n","    if 'bonds_hour' in file: return 'bonds_hour'\n","    if 'crypto_day' in file: return 'crypto_day'\n","    if 'crypto_hour' in file: return 'crypto_hour'\n","    if 'stock_day' in file: return 'stock_day'\n","    if 'stock_hour' in file: return 'stock_hour'\n","    if 'others_day' in file: return 'others_day'\n","    if 'others_hour' in file: return 'others_hour'\n","\n","    return os.path.splitext(os.path.basename(file))[0]\n","\n","def read_and_clean(file):\n","    df = pd.read_csv(file)\n","    date_col = find_date_col(df)\n","    tried = False\n","    for fmt in ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M', '%Y-%m-%d %H:%M', '%Y/%m/%d %H:%M:%S']:\n","        try:\n","            df[date_col] = pd.to_datetime(df[date_col], format=fmt, errors='raise')\n","            tried = True\n","            break\n","        except Exception:\n","            continue\n","    if not tried:\n","        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n","    # 強制 floor 到小時，避免交集失敗\n","    df[date_col] = df[date_col].dt.floor('h')\n","    df[date_col] = df[date_col].dt.tz_localize(None)\n","    df = df.rename(columns={date_col: 'DATE'})\n","    return df\n","\n","\n","dfs = []\n","for file in csv_files:\n","    df = read_and_clean(file)\n","    date_col = find_date_col(df)\n","    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n","    df = df.dropna(subset=[date_col])\n","    prefix = get_prefix(file)\n","    # 將所有非日期欄位加前綴\n","    rename = {col: f\"{col.upper()}\" for col in df.columns if col != date_col}\n","    df = df.rename(columns=rename)\n","    df = df.rename(columns={date_col: 'DATE'})\n","    dfs.append(df)\n","\n","#{prefix}_\n","\n","# df_shapes = pd.DataFrame([\n","#     {\n","#         'idx': i,\n","#         'rows': df.shape[0],\n","#         'cols': df.shape[1]\n","#     }\n","#     for i, df in enumerate(dfs)\n","# ])\n","# print(df_shapes)\n","\n","# 1) 先把 dfs 和 csv_files 对应起来，生成一个 prefix→df 的 dict\n","dfs_dict = {}\n","for file, df in zip(csv_files, dfs):\n","    prefix = get_prefix(file)       # e.g. 'bonds_day', 'bonds_hour'\n","    dfs_dict[prefix] = df\n","    print(dfs_dict[prefix].columns)\n","\n","# 2) 重组成 asset → {'day': df, 'hour': df}，方便后续两两配对\n","by_asset = defaultdict(dict)\n","\n","for prefix, df in dfs_dict.items():\n","    asset, freq = prefix.rsplit('_', 1)  # 把 'bonds_day' 分成 ['bonds','day']\n","    by_asset[asset][freq] = df\n","    #print(by_asset[asset][freq].columns)\n","    # print(by_asset[asset][freq]['DATE'].dtype)\n","\n","\n","out_dir = './output1/row_group'\n","os.makedirs(out_dir, exist_ok=True)\n","\n","# 先在外面建一个空字典，用来存放 clean 后的小时线\n","cleaned_hours = {}\n","\n","for asset, pair in by_asset.items():\n","\n","    if 'day' in pair and 'hour' in pair:\n","        print(f\"{asset}: day 有 {pair['day'].shape}，hour 有 {pair['hour'].shape}\")\n","    else:\n","        print(f\"⚠️ {asset} 缺少 day 或 hour，检查 get_prefix 是否正确\")\n","\n","    df_day  = pair['day']\n","    df_hour = pair['hour']\n","\n","    # 1) 按 DATE 生成字符串，方便比对\n","    df_day  = df_day.copy()\n","    df_hour = df_hour.copy()\n","\n","    df_day['DATE_STR']  = pd.to_datetime(df_day['DATE']).dt.strftime('%Y-%m-%d')\n","    df_hour['DATE_full'] = pd.to_datetime(df_hour['DATE'])\n","    df_hour['DATE_STR'] = df_hour['DATE_full'].dt.strftime('%Y-%m-%d')\n","\n","    # 2) 找到那些 hour 数据里，日期不在 day 里的，单独输出 exclude\n","    df_hour_exclude = df_hour[~df_hour['DATE_STR'].isin(df_day['DATE_STR'])].copy()\n","    df_hour_exclude = df_hour_exclude.drop(columns=['DATE_STR'])\n","    df_hour_exclude = df_hour_exclude.rename(columns={'DATE_full': 'DATE'})\n","    path_ex = os.path.join(out_dir, f\"{asset}_hourly_exclude.csv\")\n","    df_hour_exclude.to_csv(path_ex, index=False, encoding='utf-8-sig')\n","    print(f\"❌ {asset} – 排除掉不在日线中的小时点，共 {len(df_hour_exclude)} 行 → {path_ex}\")\n","\n","    # 3) 保留 clean 部分\n","    df_hour_clean = df_hour[df_hour['DATE_STR'].isin(df_day['DATE_STR'])].copy()\n","    df_hour_clean = df_hour_clean.drop(columns=['DATE_STR'])\n","    df_hour_clean = df_hour_clean.rename(columns={'DATE_full': 'DATE'})\n","    path_cl = os.path.join(out_dir, f\"{asset}_hourly_clean.csv\")\n","    df_hour_clean.to_csv(path_cl, index=False, encoding='utf-8-sig')\n","    print(f\"✅ {asset} – 清洗后小时线对齐日线，共 {len(df_hour_clean)} 行 → {path_cl}\")\n","\n","    # 4) 把 clean 后的 df 存进字典\n","    cleaned_hours[asset] = df_hour_clean\n","\n","# # 循环结束后，cleaned_hours 就是一个 { asset: df_hour_clean } 的字典\n","# # 你可以在此之后再统一对它们做什么，比如：\n","# for asset, df in cleaned_hours.items():\n","#     print(f\"{asset} 对齐后共有 {df.shape[0]} 行，{df.shape[1]} 列\")\n","#     # 一次性看所有列\n","#     print(df.dtypes)\n","#     print('-' * 40)\n","\n","# print(type(cleaned_hours), type(by_asset))\n","# print(\"cleaned_hours 的资产列表：\", list(cleaned_hours.keys()))\n","# print(\"by_asset 的资产列表：\", list(by_asset.keys()))\n","\n","# 输出目录\n","out_dir1 = './filtered_output'\n","os.makedirs(out_dir1, exist_ok=True)\n","\n","for asset, df_hour in cleaned_hours.items():\n","    # 1) 删除重复的列（保留第一个出现的）\n","    df_hour = df_hour.loc[:, ~df_hour.columns.duplicated()]\n","\n","    mask = (df_hour['DATE'] >= start) & (df_hour['DATE'] <= end)\n","    df_hour = df_hour.loc[mask]  # 先筛选\n","    df_hour.loc[:, 'DATE'] = pd.to_datetime(df_hour['DATE'], errors='coerce')\n","\n","    # 然后导出\n","    out_path = os.path.join(out_dir1, f\"{asset}_hour_clean_period.csv\")\n","    df_hour.to_csv(out_path, index=False, encoding='utf-8-sig')\n","    print(f\"✅ {asset} 小时数据 ({len(df_hour)} rows) 已保存到 {out_path}\")\n","\n","\n","# 2) 处理日数据\n","for asset, freq_dict in by_asset.items():\n","    df_day = freq_dict.get('day')\n","    if df_day is None:\n","        print(f\"⚠️ {asset} 没有日频数据，跳过\")\n","        continue\n","    # 确保 DATE 列是 datetime\n","    df_day['DATE'] = pd.to_datetime(df_day['DATE'])\n","    # 筛选\n","    df_filt = df_day[(df_day['DATE'] >= start) & (df_day['DATE'] <= end)]\n","    # 导出\n","    out_path = os.path.join(out_dir1, f\"{asset}_day_clean_period.csv\")\n","    df_filt.to_csv(out_path, index=False, encoding='utf-8-sig')\n","    print(f\"✅ {asset} 日线数据 ({len(df_filt)} rows) 已保存到 {out_path}\")"]}]}